"""
The ONNX to keras converter module
"""
import importlib.util
import inspect
import logging
import uuid

import keras
import keras.backend

from .customonnxlayer import onnx_custom_objects_map
from .exceptions import UnsupportedLayer, OnnxUnsupported
from .layers import AVAILABLE_CONVERTERS
import tensorflow as tf
onnx_imported = False
package_name = 'onnx'
spec = importlib.util.find_spec(package_name)
if spec is not None:
    from onnx import numpy_helper

    onnx_imported = True


def onnx_node_attributes_to_dict(args):
    """
    Parse ONNX attributes to Python dictionary
    :param args: ONNX attributes object
    :return: Python dictionary
    """

    def onnx_attribute_to_dict(onnx_attr):
        """
        Parse ONNX attribute
        :param onnx_attr: ONNX attribute
        :return: Python data type
        """
        if onnx_attr.HasField('t'):
            return numpy_helper.to_array(getattr(onnx_attr, 't'))

        for attr_type in ['f', 'i', 's']:
            if onnx_attr.HasField(attr_type):
                return getattr(onnx_attr, attr_type)

        for attr_type in ['floats', 'ints', 'strings']:
            if getattr(onnx_attr, attr_type):
                return list(getattr(onnx_attr, attr_type))

    return {arg.name: onnx_attribute_to_dict(arg) for arg in args}


def onnx_to_keras(onnx_model, input_names, name_policy=None, verbose=True, change_ordering=False, input_types=None):
    """
    Convert ONNX graph to Keras model format
    :param onnx_model: loaded ONNX model
    :param input_names: list with input names
    :param name_policy: override layer names. None, "short", "renumerate" or "attach_weights_name" (last 2 are experimental)
    :param verbose: verbose output
    :param change_ordering: change ordering to HWC (experimental)
    :return: Keras model
    """
    if not onnx_imported:
        raise OnnxUnsupported()
    # Use channels first format by default.
    keras_fmt = keras.backend.image_data_format()
    keras.backend.set_image_data_format('channels_first')

    if verbose:
        logging.basicConfig(level=logging.DEBUG)

    logger = logging.getLogger('onnx2keras')

    logger.info('Converter is called.')

    onnx_weights = onnx_model.graph.initializer
    onnx_inputs = onnx_model.graph.input
    onnx_outputs = [i.name for i in onnx_model.graph.output]
    onnx_nodes = onnx_model.graph.node

    logger.debug('List inputs:')
    for i, input in enumerate(onnx_inputs):
        logger.debug('Input {0} -> {1}.'.format(i, input.name))

    logger.debug('List outputs:')
    for i, output in enumerate(onnx_outputs):
        logger.debug('Output {0} -> {1}.'.format(i, output))

    logger.debug('Gathering weights to dictionary.')
    weights = {}
    for onnx_w in onnx_weights:
        try:
            if len(onnx_w.ListFields()) < 4:
                onnx_extracted_weights_name = onnx_w.ListFields()[1][1]
            else:
                onnx_extracted_weights_name = onnx_w.ListFields()[2][1]
            weights[onnx_extracted_weights_name] = numpy_helper.to_array(onnx_w)
        except:
            onnx_extracted_weights_name = onnx_w.ListFields()[3][1]
            weights[onnx_extracted_weights_name] = numpy_helper.to_array(onnx_w)

        logger.debug('Found weight {0} with shape {1}.'.format(
            onnx_extracted_weights_name,
            weights[onnx_extracted_weights_name].shape))

    layers = dict()
    lambda_funcs = dict()
    keras_outputs = []
    keras_inputs = []

    for i, input_name in enumerate(input_names):
        for onnx_i in onnx_inputs:
            if onnx_i.name == input_name:
                dtype = None if input_types is None else input_types[i]
                input_shape = [i.dim_value for i in onnx_i.type.tensor_type.shape.dim]
                input_shape = [shape if shape != 0 else None for shape in input_shape]
                if len(input_shape) <= 1:
                    input_tensor = keras.layers.InputLayer(input_shape=input_shape, name=input_name, dtype=dtype).output
                    layers[input_name] = input_tensor[0]
                    keras_inputs.append(input_tensor)

                else:
                    batch_size = input_shape[0]
                    input_shape = input_shape[1:]
                    if batch_size is None:
                        layers[input_name] = keras.layers.InputLayer(
                            input_shape=input_shape, name=input_name, dtype=dtype).output
                    else:
                        layers[input_name] = keras.layers.InputLayer(
                            input_shape=input_shape, name=input_name, dtype=dtype, batch_size=batch_size).output

                    keras_inputs.append(layers[input_name])

                logger.debug('Found input {0} with shape {1}'.format(input_name, input_shape))

    # Convert every operation separable
    node_names = []
    embedding_weights_mapping = {}
    for node_index, node in enumerate(onnx_nodes):
        if node.op_type == 'If':
            cond = layers[node.input[0]][0]
            if not isinstance(cond, bool) and not isinstance(cond, tf.Tensor) and keras.backend.is_keras_tensor(cond):
                # the condition in If is a KerasTensor and needs to be evlauated.
                inpt_sample = [tf.ones(inpt.shape) for inpt in keras_inputs]
                cond = keras.models.Model(keras_inputs, cond)(inpt_sample)
            if cond:
                replace_node = node.attribute[0].g.node
            else:
                replace_node = node.attribute[1].g.node
            replace_node = extract_op_node(replace_node, layers, lambda_funcs, keras_names, change_ordering, name_policy)
            replace_node.output.pop()
            for i in range(len(node.output)):
                replace_node.output.append(node.output[i])
            node = replace_node
        node_type = node.op_type
        node_params = onnx_node_attributes_to_dict(node.attribute)
        # Add global converter info:
        node_params['change_ordering'] = change_ordering
        node_params['name_policy'] = name_policy

        node_name = str(node.output[0])
        keras_names = []
        for output_index, output in enumerate(node.output):
            if name_policy == 'short':
                keras_name = keras_name_i = str(output)[:8]
                suffix = 1
                while keras_name_i in node_names:
                    keras_name_i = keras_name + '_' + str(suffix)
                    suffix += 1
                keras_names.append(keras_name_i)
            elif name_policy == 'renumerate':
                postfix = node_index if len(node.output) == 1 else "%s_%s" % (node_index, output_index)
                keras_names.append('LAYER_%s' % postfix)
            elif name_policy == 'attach_weights_name':
                attached_weights_names = []
                for node_input in node.input:
                    if node_input in weights:
                        weight_name = ".".join(node_input.split(".")[:-1])
                        attached_weights_names.append(weight_name)
                set_weights_names = set(attached_weights_names)
                set_weights_names = "__".join(set_weights_names)
                layer_name = output.replace(":", "_")
                while not (str.isalpha(layer_name[0]) or str.isdigit(layer_name[0]) or layer_name[0] == "."):
                    layer_name = layer_name[1:]

                if layer_name == "":
                    layer_name = str(uuid.uuid4())[:10]

                if set_weights_names:
                    layer_name = f"{layer_name}__{set_weights_names}"

                keras_names.append(layer_name)
            else:
                output = output.replace(":", "_")
                keras_names.append(output)
        keras_names = [k.lstrip("/") for k in keras_names]
        if len(node.output) != 1:
            logger.warning('Trying to convert multi-output node')
            node_params['_outputs'] = list(node.output)
            node_names.extend(keras_names)
        else:
            keras_names = keras_names[0]
            node_names.append(keras_names)

        logger.debug('######')
        logger.debug('...')
        logger.debug('Converting ONNX operation')
        logger.debug('type: %s', node_type)
        logger.debug('node_name: %s', node_name)
        logger.debug('node_params: %s', node_params)
        logger.debug('...')

        logger.debug('Check if all inputs are available:')
        if len(node.input) == 0 and node_type != 'Constant':
            raise AttributeError('Operation doesn\'t have an input. Aborting.')
        for i, node_input in enumerate(node.input):
            logger.debug('Check input %i (name %s).', i, node_input)

            # for case of weights sharing, map the shared weights to determine
            # if a Gather layer is an embedding layer
            if node_type == 'Identity' and node_input in weights:
                embedding_weights_mapping[node_name] = node_input

            # check conditions for embedding layer
            is_in_weights = node_input in weights  # is this node input in weights
            is_mapped_to_weights = embedding_weights_mapping.get(node_input, '') in weights  # is this node inputs weights are shared with other input
            is_embedding = (is_in_weights or is_mapped_to_weights) and i == 0  # if either is true this layer is a possible embedding layer

            # if a layer is of type Gather and its input is in weights (or mapped to a weights input)
            # it's an embedding layer
            if node_type == "Gather" and is_embedding:
                node_params['is_embedding'] = True

            if node_input not in layers:
                logger.debug('The input not found in layers / model inputs.')
                if node_input in weights:
                    logger.debug('Found in weights, add as a numpy constant.')
                    layers[node_input] = weights[node_input]
                else:
                    if node_input == "" and node_type in ('Pad', 'Resize', 'Clip', 'LSTM'):
                        continue
                    else:
                        raise AttributeError('Current node is not in weights / model inputs / layers.')
        else:
            logger.debug('... found all, continue')

        keras.backend.set_image_data_format('channels_first')
        try:
            layer_converter_func = AVAILABLE_CONVERTERS[node_type]
        except KeyError:
            raise UnsupportedLayer(node_type)
        layer_converter_func(
            node,
            node_params,
            layers,
            lambda_funcs,
            node_name,
            keras_names
        )
        if isinstance(keras_names, list):
            keras_names = keras_names[0]

        try:
            logger.debug('Output TF Layer -> ' + str(layers[keras_names]))
        except KeyError:
            pass

    # Check for terminal nodes
    for layer in onnx_outputs:
        if layer in layers:
            keras_outputs.append(layers[layer])

    # Create model
    model = keras.models.Model(inputs=keras_inputs, outputs=keras_outputs)

    if change_ordering:
        change_ord_axes_map = {
            3: 2,
            1: 3,
            -1: 1
        }

        import numpy as np
        conf = model.get_config()

        for layer in conf['layers']:
            if layer['config'] and 'shared_axes' in layer['config']:
                # TODO: check axes first (if it's not 4D tensor)
                layer['config']['shared_axes'] = [1, 2]

            if layer['config'] and 'batch_input_shape' in layer['config']:
                layer['config']['batch_input_shape'] = \
                    tuple(np.reshape(np.array(
                        [
                            [None] +
                            list(layer['config']['batch_input_shape'][2:][:]) +
                            [layer['config']['batch_input_shape'][1]]
                        ]), -1
                    ))
            if layer['config'] and 'target_shape' in layer['config']:
                if len(list(layer['config']['target_shape'][1:][:])) > 0:
                    layer['config']['target_shape'] = \
                        tuple(np.reshape(np.array(
                            list(layer['config']['target_shape'][1:]) +
                            [layer['config']['target_shape'][0]]
                        ), -1), )

            if layer['config'] and 'data_format' in layer['config']:
                layer['config']['data_format'] = 'channels_last'
            if layer['config'] and 'axis' in layer['config']:
                axis = layer['config']['axis']
                # BatchNorm wrap axis with ListWrapper instead single INT value
                if isinstance(axis, (tuple, list)):
                    axis = axis[0]
                layer['config']['axis'] = change_ord_axes_map.get(axis, layer['config']['axis'])

        for layer in conf['layers']:
            if 'function' in layer['config'] and layer['config']['function'][1] is not None:
                kerasf = list(layer['config']['function'])
                dargs = list(kerasf[1])
                func = lambda_funcs.get(layer['name'])

                if func:
                    # ReduceSum operation has 'axis' param as array of ints. When onnx uses ReduceSum
                    # to reproduce SoftMax - dargs become something like [[1]] (list of lists)
                    # that why we handle collections.Iterable
                    if len(dargs) > 1 or isinstance(dargs[0], (tuple, list)):
                        params = inspect.signature(func).parameters
                        i = list(params.keys()).index('axes') if ('axes' in params) else -1

                        if i > 0:
                            i -= 1
                            axes = list(range(len(dargs[i].shape)))
                            axes = axes[0:1] + axes[2:] + axes[1:2]
                            dargs[i] = np.transpose(dargs[i], axes)

                        i = list(params.keys()).index('axis') if ('axis' in params) else -1

                        if i > 0:
                            i -= 1
                            axis = np.array(dargs[i])
                            axes_map = np.array([0, 3, 1, 2])
                            # to list because some tf operations check only for core python types (e.g tf.norm)
                            dargs[i] = axes_map[axis].tolist()
                    else:
                        # if map exits will change else will remain the same
                        dargs[0] = change_ord_axes_map.get(dargs[0], dargs[0])

                kerasf[1] = tuple(dargs)
                layer['config']['function'] = tuple(kerasf)

        keras.backend.set_image_data_format('channels_last')
        model_tf_ordering = keras.models.Model.from_config(conf, custom_objects=onnx_custom_objects_map)

        for dst_layer, src_layer, conf in zip(model_tf_ordering.layers, model.layers, conf['layers']):
            W = src_layer.get_weights()
            # TODO: check axes first (if it's not 4D tensor)
            if conf['config'] and 'shared_axes' in conf['config']:
                W[0] = W[0].transpose(1, 2, 0)
            dst_layer.set_weights(W)

        model = model_tf_ordering

    keras.backend.set_image_data_format(keras_fmt)

    return model



def extract_op_node(node_graph, layers, lambda_funcs, keras_names, change_ordering, name_policy):
    op_node = None
    for node_i, node in enumerate(node_graph):
        if node.op_type == 'Constant':
            node_params = onnx_node_attributes_to_dict(node.attribute)
            # Add global converter info:
            node_params['change_ordering'] = change_ordering
            node_params['name_policy'] = name_policy
            node_name = str(node.output[0])

            AVAILABLE_CONVERTERS[node.op_type](
                node,
                node_params,
                layers,
                lambda_funcs,
                node_name,
                keras_names
            )
        else:       # op type
            if op_node is not None:
                raise NotImplementedError('Not Implemented: inner graph in If node with multiple operator nodes')
            op_node = node
    if op_node is None:
        raise NotImplementedError('Something is off with If node')
    return op_node


